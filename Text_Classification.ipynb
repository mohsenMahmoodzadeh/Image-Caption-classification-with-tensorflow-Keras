{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mohsenMahmoodzadeh/Image-Caption-classification-with-tensorflow-Keras/blob/master/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfkX79ko-5zs"
   },
   "source": [
    "# Text Classification with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5Zlntq--_L0"
   },
   "source": [
    "In this notebook, we build a keras model to classify a text dataset with 2658 train and 2658 test senetences. \n",
    "\n",
    "We use Tensorflow and Keras for implementation of our model. \n",
    "\n",
    "We also use scikit-learn library for some ancillary affairs such as confusion matrix, accuracy score, classification report, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Em-mtxWC8cBI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D,Activation,MaxPool2D,Dropout,Dense,Flatten,Input,BatchNormalization\n",
    "from tensorflow.keras.models import Sequential,load_model,Model\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore','FutureWarning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lvhg4C5ZCcO1"
   },
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0 : 'aeroplane', 1 : 'bicycle', 2 : 'bird', 3 : 'boat', \n",
    "    4 : 'bus', 5 : 'car', 6 : 'cat', 7 : 'chair', 8 : 'cow',\n",
    "    9 : 'diningtable', 10 : 'dog', 11 : 'horse', 12 : 'motorbike',\n",
    "    13 : 'person', 14 : 'pottedplant', 15 : 'sheep', 16 : 'sofa',\n",
    "    17 : 'train', 18 : 'tvmonitor'\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XfhBNVHTChfd"
   },
   "outputs": [],
   "source": [
    "CATEGORIES = list(label_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-5Hv_Fks6Rx",
    "outputId": "411004ab-2552-46f2-f49b-cd278235e936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "# !cp '/content/glove.6B.zip' \"drive/My Drive/glove.6B.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fP-MqnIXCmAj"
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KCB9uJUoCn7n"
   },
   "outputs": [],
   "source": [
    "def prepare_train_data(TRAIN_DATA_DIR, CATEGORIES):\n",
    "    train_data = []\n",
    "    path = \"\"\n",
    "    for category in CATEGORIES:\n",
    "        temp = []\n",
    "        path = os.path.join(TRAIN_DATA_DIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for filename in os.listdir(path):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            try:\n",
    "                lines = read_file(file_path)\n",
    "                for line in lines:\n",
    "                    temp.append(line)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "        \n",
    "        for line in temp:\n",
    "            train_data.append((line, class_num))\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Plh5K81VGwOh"
   },
   "outputs": [],
   "source": [
    "def prepare_test_data(TEST_DATA_DIR, CATEGORIES):\n",
    "    test_data = []\n",
    "    path = \"\"\n",
    "    for category in CATEGORIES:\n",
    "        temp = []\n",
    "        path = os.path.join(TRAIN_DATA_DIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for filename in os.listdir(path):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            file = file_path\n",
    "            try:\n",
    "                lines = read_file(file)\n",
    "                for line in lines:\n",
    "                    temp.append(line)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "        \n",
    "        for line in temp:\n",
    "            test_data.append((line, class_num))\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B6ZRJknYDi3D"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = \"/content/drive/MyDrive/dataset/train/sentences/\"\n",
    "TEST_DATA_DIR = \"/content/drive/MyDrive/dataset/test/sentences/\"\n",
    "CATEGORIES = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \n",
    "              \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\"sofa\", \"train\", \"tvmonitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9qy6937lEegR"
   },
   "outputs": [],
   "source": [
    "training_data = prepare_train_data(TRAIN_DATA_DIR, CATEGORIES)\n",
    "test_data = prepare_test_data(TEST_DATA_DIR, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyvatVkb_OsT"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy requests nlpaug\n",
    "# import nlpaug.augmenter.word as naw\n",
    "# aug = naw.WordEmbsAug(model_type='glove', model_path=\"/content/glove.6B.50d.txt\", action=\"insert\")\n",
    "# aug_training_data = [(aug.augment(training_data[i][0]), training_data[i][1]) for i in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l_jzrMXQFquo"
   },
   "outputs": [],
   "source": [
    "random.shuffle(training_data)\n",
    "# random.shuffle(aug_training_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DB3oDGYSN1PV"
   },
   "outputs": [],
   "source": [
    "def extract_X_and_y(data):\n",
    "  data_dict = {}\n",
    "  X = []\n",
    "  y = []\n",
    "  for example in data:\n",
    "    X.append(example[0])\n",
    "    y.append(example[1])\n",
    "  data_dict['X'] = X\n",
    "  data_dict['y'] = y\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d3dY9M9s0V72"
   },
   "outputs": [],
   "source": [
    "# training_dict = extract_X_and_y(aug_training_data)\n",
    "training_dict = extract_X_and_y(training_data)\n",
    "# X = training_dict['X']\n",
    "# y = training_dict['y']\n",
    "X_train = training_dict['X']\n",
    "y_train = training_dict['y']\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HuNUSFnvOwkX"
   },
   "outputs": [],
   "source": [
    "test_dict = extract_X_and_y(test_data)\n",
    "X_test = test_dict['X']\n",
    "y_test = test_dict['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsORK5rYZzw8"
   },
   "outputs": [],
   "source": [
    "# def clean_data(text):  \n",
    "#   word_tokens= text.lower().split()\n",
    "#   le=WordNetLemmatizer()\n",
    "#   stop_words= set(stopwords.words(\"english\"))     \n",
    "#   word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "  \n",
    "#   cleaned_data=\" \".join(word_tokens)\n",
    "#   return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roDL_c-WAeqq",
    "outputId": "9e003fe2-5488-4396-d88a-6467660290c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riZ30DgaHs3z"
   },
   "outputs": [],
   "source": [
    "# cleaned_X_train = []\n",
    "# for tr_example in X_train:\n",
    "#   cleaned_data = clean_data(tr_example)\n",
    "#   cleaned_X_train.append(cleaned_data)\n",
    "#   # tokenizer.fit_on_texts(tr_example.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lucSUp3XBNPy"
   },
   "outputs": [],
   "source": [
    "# cleaned_X_val = []\n",
    "# for val_example in X_val:\n",
    "#   cleaned_data = clean_data(tr_example)\n",
    "#   cleaned_X_val.append(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9s7aR8wCdLk"
   },
   "outputs": [],
   "source": [
    "# cleaned_X_test = []\n",
    "# for test_example in X_test:\n",
    "#   cleaned_data = clean_data(test_example)\n",
    "#   cleaned_X_test.append(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "depGzlJ735pW"
   },
   "outputs": [],
   "source": [
    "vocab_size = 2304 \n",
    "oov_token = '<OOV>' #OOV = Out of Vocabulary\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
    "maxlen = 100\n",
    "padding_type = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ijIQLfpumzVG"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(data):\n",
    "  STOPWORDS= set(stopwords.words(\"english\"))     \n",
    "  no_sw_data = []\n",
    "  for example in data:\n",
    "      for word in STOPWORDS:\n",
    "          token = ' ' + word + ' '\n",
    "          example = example.replace(token, ' ')\n",
    "          example = example.replace(' ', ' ')\n",
    "      no_sw_data.append(example)\n",
    "  return no_sw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JOTs6YMjNQfA"
   },
   "outputs": [],
   "source": [
    "X_train = remove_stopwords(X_train)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen, padding=padding_type)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LEyFpB9076O"
   },
   "outputs": [],
   "source": [
    "# X_val = remove_stopwords(X_val)\n",
    "# tokenizer.fit_on_texts(X_val)\n",
    "# X_val = tokenizer.texts_to_sequences(X_val)\n",
    "# X_val = pad_sequences(X_val, maxlen=maxlen, padding=padding_type)\n",
    "# y_val_cat = tf.keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "QuoY1Q9DPKtY"
   },
   "outputs": [],
   "source": [
    "X_test = remove_stopwords(X_test)\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen, padding=padding_type)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pXEUzgaPWDdF"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('/content/drive/MyDrive/glove.6B.zip', 'r')\n",
    "zip_ref.extractall('/content/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-7ft8x0O5jC"
   },
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# sentences= cleaned_X_train + cleaned_X_val + cleaned_X_test\n",
    "# w2v_model=gensim.models.Word2Vec(sentences= sentences, size=300, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkxbIHBxDvdq"
   },
   "outputs": [],
   "source": [
    "# w2v_model.train(sentences,epochs=10,total_examples=len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsXPDZW9O_K5"
   },
   "outputs": [],
   "source": [
    "# vocab=w2v_model.wv.vocab\n",
    "# print(\"The total number of words are : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BV0LwfvFTmZ"
   },
   "outputs": [],
   "source": [
    "# vocab=list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxmtXet2GZVL"
   },
   "outputs": [],
   "source": [
    "# word_vec_dict={}\n",
    "# for word in vocab:\n",
    "#   word_vec_dict[word]=w2v_model.wv.get_vector(word)\n",
    "# print(\"The no of key-value pairs : \",len(word_vec_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zUdS0GEIBbR"
   },
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(sentences)\n",
    "# vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4i66n8f2Gbh1"
   },
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(cleaned_X_rtain)\n",
    "# X_train = tokenizer.texts_to_sequences(cleaned_X_rtain)\n",
    "# X_train = pad_sequences(X_train, maxlen=maxlen, padding=padding_type)\n",
    "# y_train_cat = tf.keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oOHUHEtH1Nh"
   },
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(cleaned_X_val)\n",
    "# X_val = tokenizer.texts_to_sequences(cleaned_X_val)\n",
    "# X_val = pad_sequences(X_val, maxlen=maxlen, padding=padding_type)\n",
    "# y_val_cat = tf.keras.utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rdujeRVIVSo"
   },
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(cleaned_X_test)\n",
    "# X_test = tokenizer.texts_to_sequences(cleaned_X_test)\n",
    "# X_test = pad_sequences(X_test, maxlen=maxlen, padding=padding_type)\n",
    "# y_test_cat = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LCqAuhbcXH_Y"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "M4NuBsiZX_Lb"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('/content/glove.6B.50d.txt',tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x6OlYI1YNFA",
    "outputId": "976e5f63-a6c3-470c-df89-b183aa3d434d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800347222222222"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UtG5TYeiYTZC"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, maxlen):\n",
    "  num_classes= 19\n",
    "  model = Sequential()\n",
    "  \n",
    "  model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=maxlen, \n",
    "                            trainable=True\n",
    "                            ))\n",
    "  model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "  model.add(layers.GlobalMaxPool1D())\n",
    "  model.add(layers.Dropout(0.3))\n",
    "  model.add(layers.Dense(20, activation='relu'))\n",
    "  # model.add(layers.Dropout(0.1))\n",
    "  model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txp4fhZ4ZyW3"
   },
   "outputs": [],
   "source": [
    "# plt.style.use('ggplot')\n",
    "\n",
    "# def plot_history(history):\n",
    "#     acc = history.history['acc']\n",
    "#     val_acc = history.history['val_acc']\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "#     x = range(1, len(acc) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(x, acc, 'b', label='Training acc')\n",
    "#     plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "#     plt.title('Training and validation accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(x, loss, 'b', label='Training loss')\n",
    "#     plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "#     plt.title('Training and validation loss')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZqbuM6RHvTd"
   },
   "outputs": [],
   "source": [
    "# print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dw1Xdt-7Z_T0",
    "outputId": "9001bd35-f55a-4561-9890-2f74fb42aefe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 3s 26ms/step - loss: 2.7239 - accuracy: 0.1442 - val_loss: 2.3580 - val_accuracy: 0.2895\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 2.0575 - accuracy: 0.3821 - val_loss: 1.7830 - val_accuracy: 0.4887\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 1.5048 - accuracy: 0.5497 - val_loss: 1.4056 - val_accuracy: 0.5677\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 1.1266 - accuracy: 0.6710 - val_loss: 1.2765 - val_accuracy: 0.6316\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.8961 - accuracy: 0.7391 - val_loss: 1.1487 - val_accuracy: 0.6617\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.7211 - accuracy: 0.7964 - val_loss: 1.0397 - val_accuracy: 0.6917\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.6136 - accuracy: 0.8278 - val_loss: 1.0133 - val_accuracy: 0.7030\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 2s 24ms/step - loss: 0.5103 - accuracy: 0.8570 - val_loss: 1.0404 - val_accuracy: 0.6842\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 2s 23ms/step - loss: 0.4254 - accuracy: 0.8842 - val_loss: 0.9750 - val_accuracy: 0.7068\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.3541 - accuracy: 0.9051 - val_loss: 0.9788 - val_accuracy: 0.7218\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, embedding_dim, maxlen)\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "history = model.fit(X_train, y_train_cat,\n",
    "                    epochs=epochs,\n",
    "                    # validation_data=(X_val, y_val_cat),\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ty3Tk0uCv9et"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpNz3rszv-WS",
    "outputId": "0dcca4dd-e724-41ca-f600-fc370f9fdab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[138   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0 137   0   0   0   0   0   1   0   1   0   0   0   1   0   0   0   0\n",
      "    0]\n",
      " [  0   0 140   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  1   0   0 138   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0]\n",
      " [  3   3   0   0 131   1   0   0   0   0   0   0   0   2   0   0   0   0\n",
      "    0]\n",
      " [  5   3   0   0   6 117   0   1   1   1   0   0   4   2   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0 137   0   1   0   1   0   0   0   1   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   1 125   0   5   0   1   0   1   2   0   3   0\n",
      "    2]\n",
      " [  0   0   2   0   0   0   0   0 133   0   2   2   0   0   0   0   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   5   0 120   1   0   0   5   3   0   3   0\n",
      "    3]\n",
      " [  0   0   0   0   0   0   0   2   0   0 132   0   0   0   2   4   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 136   0   4   0   0   0   0\n",
      "    0]\n",
      " [  0   5   0   0   0   3   0   1   1   0   0   0 128   1   1   0   0   0\n",
      "    0]\n",
      " [  0   0   0   3   1   0   0   4   0   2   0   0   0 129   1   0   0   0\n",
      "    0]\n",
      " [  0   1   3   3   0   1   1   4   0   0   6   0   0   3 115   0   2   0\n",
      "    1]\n",
      " [  0   1   0   0   0   0   0   0   1   0   3   0   0   0   0 135   0   0\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   1   0   0   2   2   0 132   0\n",
      "    2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 139\n",
      "    0]\n",
      " [  0   0   0   0   0   0   0   6   0   2   0   0   0   0   1   0   2   0\n",
      "  129]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   aeroplane       0.94      0.99      0.96       140\n",
      "     bicycle       0.91      0.98      0.94       140\n",
      "        bird       0.97      1.00      0.98       140\n",
      "        boat       0.95      0.99      0.97       139\n",
      "         bus       0.95      0.94      0.94       140\n",
      "         car       0.96      0.84      0.89       140\n",
      "         cat       0.99      0.98      0.98       140\n",
      "       chair       0.83      0.89      0.86       140\n",
      "         cow       0.97      0.96      0.96       139\n",
      " diningtable       0.92      0.86      0.89       140\n",
      "         dog       0.90      0.94      0.92       140\n",
      "       horse       0.98      0.97      0.97       140\n",
      "   motorbike       0.97      0.91      0.94       140\n",
      "      person       0.86      0.92      0.89       140\n",
      " pottedplant       0.90      0.82      0.86       140\n",
      "       sheep       0.97      0.96      0.97       140\n",
      "        sofa       0.92      0.94      0.93       140\n",
      "       train       1.00      0.99      1.00       140\n",
      "   tvmonitor       0.94      0.92      0.93       140\n",
      "\n",
      "    accuracy                           0.94      2658\n",
      "   macro avg       0.94      0.94      0.94      2658\n",
      "weighted avg       0.94      0.94      0.94      2658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, \n",
    "                            y_pred, \n",
    "                            target_names=CATEGORIES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "347bcN06aaN4",
    "outputId": "64828ebd-7adf-4f89-92db-74c11bda0010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9372\n",
      "Testing Accuracy:  0.9372\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train_cat, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test_cat, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INbYFY5BKScO"
   },
   "outputs": [],
   "source": [
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "# inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "# targets = np.concatenate((y_train_cat, y_test_cat), axis=0)\n",
    "# fold_no = 1\n",
    "# acc_per_fold = []\n",
    "# loss_per_fold = []\n",
    "# learning_rate= 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtSbsB7SvOF7"
   },
   "outputs": [],
   "source": [
    "# for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "#   model = create_model(vocab_size, embedding_dim, maxlen)\n",
    "\n",
    "#   model.compile(loss=categorical_crossentropy, optimizer=Adam(lr= learning_rate), metrics=['accuracy'])\n",
    "\n",
    "#   print('------------------------------------------------------------------------')\n",
    "#   print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "#   history = model.fit(inputs[train], targets[train],\n",
    "#               batch_size=batch_size,\n",
    "#               validation_data=(X_val, y_val_cat),\n",
    "#               epochs=epochs)\n",
    "\n",
    "#   scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "#   print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "#   acc_per_fold.append(scores[1] * 100)\n",
    "#   loss_per_fold.append(scores[0])\n",
    "\n",
    "#   fold_no = fold_no + 1\n",
    "\n",
    "\n",
    "# print('------------------------------------------------------------------------')\n",
    "# print('Score per fold')\n",
    "# for i in range(0, len(acc_per_fold)):\n",
    "#   print('------------------------------------------------------------------------')\n",
    "#   print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "# print('------------------------------------------------------------------------')\n",
    "# print('Average scores for all folds:')\n",
    "# print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "# print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "# print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFROToeVF5-q"
   },
   "outputs": [],
   "source": [
    "# def cross_validate(X, y, K = 5, **kwargs):\n",
    "#     scores = []\n",
    "#     histories = []\n",
    "#     for train, test in KFold(n_splits=K, shuffle=True).split(X,y):\n",
    "        \n",
    "#         model = create_model(vocab_size, embedding_dim, maxlen)\n",
    "#         model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate= learning_rate), metrics=['accuracy'])\n",
    "        \n",
    "#         histories.append(model.fit(X[train], y[train],\n",
    "#                                    validation_data = (X_val, y_val_cat),\n",
    "#                                    epochs=epochs,\n",
    "#                                    batch_size= batch_size,\n",
    "#                                    **kwargs).history)\n",
    "        \n",
    "#         scores.append(model.evaluate(X[test], y[test], verbose = 0))\n",
    "#     print(\"average test loss: \", np.asarray(scores)[:,0].mean())\n",
    "#     print(\"average test accuracy: \", np.asarray(scores)[:,1].mean())\n",
    "    \n",
    "#     return scores, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6qpckvQF8Mo"
   },
   "outputs": [],
   "source": [
    "# scores, histories = cross_validate(inputs, targets, K = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX5bHxpeGosH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFp4HQYZVUUs"
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mSPb7sIVW-D"
   },
   "source": [
    "1. https://realpython.com/python-keras-text-classification/\n",
    "2. https://stackoverflow.com/questions/50060241/how-to-use-glove-word-embeddings-file-on-google-colaboratory\n",
    "3. https://nlp.stanford.edu/projects/glove/\n",
    "4. https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/\n",
    "5. https://stackoverflow.com/questions/45117295/what-is-the-relation-between-validation-data-and-validation-split-in-keras-fit\n",
    "6. https://towardsdatascience.com/addressing-the-difference-between-keras-validation-split-and-sklearn-s-train-test-split-a3fb803b733\n",
    "7. https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff\n",
    "8. https://github.com/makcedward/nlpaug\n",
    "9. https://www.depends-on-the-definition.com/guide-to-word-vectors-with-gensim-and-keras/\n",
    "10. https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR6SyPg9VV2S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOwJojJevtnIJp9LgpZr1Se",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Text Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
